{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Conceptual Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was mentioned in this chapter that a cubic regression spline with\n",
    "one knot at $ξ$ can be obtained using a basis of the form $x, x^2, x^3, (x− ξ)^3_+$, where $(x− ξ)^3_+ = (x− ξ)^3$ if $x > ξ$ and equals 0 otherwise.\n",
    "We will now show that a function of the form\n",
    "$$f(x) = β_0 + β_1x + β_2x^2 + β_3x^3 + β_4(x− ξ)^3_+$$\n",
    "is indeed a cubic regression spline, regardless of the values of $β_0, β_1, β_2, β_3, β_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a cubic polynomial \n",
    "$$f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3$$\n",
    "such that $f (x) = f_1(x)$ for all $x ≤ ξ$. Express $a_1, b_1, c_1, d_1$ in terms of $β_0, β_1, β_2, β_3, β_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a cubic polynomial\n",
    "$$f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3$$\n",
    "such that $f (x) = f_2(x)$ for all $x > ξ$. Express $a_2, b_2, c_2, d_2$ in terms of $β_0, β_1, β_2, β_3, β_4$. We have now established that $f (x)$ is a piecewise polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $f_1(ξ) = f_2(ξ)$. That is, $f (x)$ is continuous at $ξ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $f'_1(ξ) = f'_2(ξ)$. That is, $f'(x)$ is continuous at $ξ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $f''_1 (ξ) = f''_2 (ξ)$. That is, $f''(x)$ is continuous at $ξ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore, $f (x)$ is indeed a cubic spline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts (d) and (e) of this problem require knowledge of single-variable calculus. As a reminder, given a cubic polynomial\n",
    "$f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3$, the first derivative takes the form $f'_1(x) = b_1 + 2c_1x + 3d_1x^2$ and the second derivative takes the form $f''_1 (x) = 2c_1 + 6d_1x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that a curve $\\hat{g}$ is computed to smoothly fit a set of $n$ points using the following formula:\n",
    "$$ \\hat{g} = \\text{ arg min}_g \\large(\\sum_{i=1}^n(y_i-g(x_i))^2 + \\lambda \\int[g^{(m)}(x)]^2 dx\\large),$$\n",
    "where $g^{(m)}$ represents the $m$th derivative of $g$ (and $g^{(0)} = g$). Provide example sketches of $\\hat{g}$ in each of the following scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda =0, m =3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we fit a curve with basis functions $b_1(X) = X, b_2(X) =(X− 1)^2 I(X ≥ 1)$. (Note that $I(X ≥ 1)$ equals 1 for $X ≥ 1$ and 0 otherwise.) We fit the linear regression model\n",
    "$$Y= β_0 + β_1b_1(X) + β_2b_2(X) + ϵ,$$\n",
    "and obtain coefficient estimates $\\hat{β}_0 = 1, \\hat{β}_1 = 1, \\hat{β}_2 =−2$. Sketch the estimated curve between $X=−2 $ and $X = 2$. Note the intercepts, slopes, and other relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we fit a curve with basis functions $b_1(X) = I(0 ≤ X ≤ 2)−(X− 1)I(1 ≤ X ≤ 2), b2(X) = (X− 3)I(3 ≤ X ≤ 4) + I(4 < X ≤ 5).$\n",
    "We fit the linear regression model\n",
    "$$Y= β_0 + β_1b_1(X) + β_2b_2(X) + ϵ,$$\n",
    "and obtain coefficient estimates $\\hat{β}_0 = 1,\\hat{β}_1 = 1, \\hat{β}_2 = 3.$ Sketch the estimated curve between $X=−2$ and $X = 6$. Note the intercepts, slopes, and other relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two curves, $\\hat{g}_1$ and $\\hat{g}_2$ defined by\n",
    "$$ \\hat{g}_1 = \\text{ arg min}_g \\large(\\sum_{i=1}^n(y_i-g(x_i))^2 + \\lambda \\int[g^{(3)}(x)]^2 dx\\large),$$\n",
    "$$ \\hat{g}_2 = \\text{ arg min}_g \\large(\\sum_{i=1}^n(y_i-g(x_i))^2 + \\lambda \\int[g^{(4)}(x)]^2 dx\\large),$$\n",
    "where $g^{(m)}$ represents the $m$th derivative of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $λ → ∞$, will $\\hat{g}_1$ or $\\hat{g}_2$ have the smaller training RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $λ → ∞$, will $\\hat{g}_1$ or $\\hat{g}_2$ have the smaller test RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $λ =0$, will $\\hat{g}_1$ or $\\hat{g}_2$ have the smaller training and test RSS?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
