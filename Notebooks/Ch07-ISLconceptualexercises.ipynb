{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Conceptual Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was mentioned in this chapter that a cubic regression spline with\n",
    "one knot at $ξ$ can be obtained using a basis of the form $x, x^2, x^3, (x− ξ)^3_+$, where $(x− ξ)^3_+ = (x− ξ)^3$ if $x > ξ$ and equals 0 otherwise.\n",
    "We will now show that a function of the form\n",
    "$$f(x) = β_0 + β_1x + β_2x^2 + β_3x^3 + β_4(x− ξ)^3_+$$\n",
    "is indeed a cubic regression spline, regardless of the values of $β_0, β_1, β_2, β_3, β_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a cubic polynomial \n",
    "$$f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3$$\n",
    "such that $f (x) = f_1(x)$ for all $x ≤ ξ$. Express $a_1, b_1, c_1, d_1$ in terms of $β_0, β_1, β_2, β_3, β_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $(x-\\xi)^3_+$ is 0 for all $x \\leq \\xi$, we can just let $a_1=\\beta_0$, $b_1=\\beta_1$, $c_1 = \\beta_2$ and $d_1 = \\beta_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a cubic polynomial\n",
    "$$f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3$$\n",
    "such that $f (x) = f_2(x)$ for all $x > ξ$. Express $a_2, b_2, c_2, d_2$ in terms of $β_0, β_1, β_2, β_3, β_4$. We have now established that $f (x)$ is a piecewise polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $(x-\\xi)^3_+=(x-\\xi)^3$ for all $x > \\xi$, we must let $a_2=\\beta_0-\\beta_4\\xi^3$, $b_2=\\beta_1+3 \\beta_4 \\xi^2$, $c_2 = \\beta_2-3 \\beta_4 \\xi$ and $d_1 = \\beta_3+\\beta_4$, because $$f_2(x) = β_0 + β_1x + β_2x^2 + β_3x^3 + β_4(x− ξ)^3 $$\n",
    "$$=  β_0 + β_1x + β_2x^2 + β_3x^3 + \\beta_4(x^3 -3\\xi x^2 +3 \\xi^2 x -\\xi^3)$$\n",
    "$$ = β_0 + β_1x + β_2x^2 + β_3x^3 + \\beta_4x^3 -3\\beta_4 \\xi x^2 +3 \\beta_4 \\xi^2 x - \\beta_4 \\xi^3$$\n",
    "$$ = (β_0 - \\beta_4 \\xi^3) + (β_1+3 \\beta_4 \\xi^2 )x + (β_2-3\\beta_4 \\xi )x^2 + (β_3+ \\beta_4)x^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $f_1(ξ) = f_2(ξ)$. That is, $f (x)$ is continuous at $ξ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_2(\\xi)=(β_0 - \\beta_4 \\xi^3) + (β_1+3 \\beta_4 \\xi^2 )\\xi + (β_2-3\\beta_4 \\xi )(\\xi)^2 + (β_3+ \\beta_4)(\\xi)^3$$\n",
    "$$= (β_0 - \\beta_4 \\xi^3) + (β_1\\xi+3 \\beta_4 \\xi^3 ) + (β_2\\xi^2-3\\beta_4 \\xi^3 ) + (β_3\\xi^3+ \\beta_4\\xi^3)$$\n",
    "$$= β_0 - \\beta_4 \\xi^3 + β_1\\xi+3 \\beta_4 \\xi^3  + β_2\\xi^2-3\\beta_4 \\xi^3  + β_3\\xi^3+ \\beta_4\\xi^3$$\n",
    "$$= β_0 + β_1\\xi  + β_2\\xi^2  + β_3\\xi^3$$\n",
    "$$=f_1(\\xi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $f'_1(ξ) = f'_2(ξ)$. That is, $f'(x)$ is continuous at $ξ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'_2(x) = (β_1+3 \\beta_4 \\xi^2 ) + 2(β_2-3\\beta_4 \\xi )x + 3(β_3+ \\beta_4)x^2$$\n",
    "$$f'_2(\\xi) = (β_1+3 \\beta_4 \\xi^2 ) + 2(β_2-3\\beta_4 \\xi )\\xi + 3(β_3+ \\beta_4)(\\xi)^2$$\n",
    "$$= (β_1+3 \\beta_4 \\xi^2 ) + 2(β_2\\xi-3\\beta_4 \\xi^2 ) + 3(β_3(\\xi)^2+ \\beta_4(\\xi)^2)$$\n",
    "$$= β_1+3 \\beta_4 \\xi^2 + 2β_2\\xi-6\\beta_4 \\xi^2 + 3β_3(\\xi)^2+ 3\\beta_4(\\xi)^2$$\n",
    "$$= β_1+ 2β_2\\xi + 3β_3(\\xi)^2+$$\n",
    "$$ = f'_1(\\xi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $f''_1 (ξ) = f''_2 (ξ)$. That is, $f''(x)$ is continuous at $ξ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f''_2(x) = 2(β_2-3\\beta_4 \\xi ) + 6(β_3+ \\beta_4)x$$\n",
    "$$f''_2(\\xi) = 2(β_2-3\\beta_4 \\xi ) + 6(β_3+ \\beta_4)\\xi$$\n",
    "$$= 2β_2-6\\beta_4 \\xi + 6β_3\\xi+ 6\\beta_4\\xi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore, $f (x)$ is indeed a cubic spline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that a curve $\\hat{g}$ is computed to smoothly fit a set of $n$ points using the following formula:\n",
    "$$ \\hat{g} = \\text{ arg min}_g \\large(\\sum_{i=1}^n(y_i-g(x_i))^2 + \\lambda \\int[g^{(m)}(x)]^2 dx\\large),$$\n",
    "where $g^{(m)}$ represents the $m$ th derivative of $g$ (and $g^{(0)} = g$). Provide example sketches of $\\hat{g}$ in each of the following scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $λ → ∞$, $g$ will be perfectly smooth—it will just be a straight line that passes as closely as possible to the training points.\n",
    "In fact, in this case, $g$ will be the linear least squares line, since the loss function (the sum) amounts to minimizing the residual sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = \\infty, m =3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda =0, m =3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $λ = 0$, then the penalty term in the expression for $\\hat{g}$ has no effect (the integral), and so the function g will be very jumpy and will exactly interpolate the training observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we fit a curve with basis functions $b_1(X) = X, b_2(X) =(X− 1)^2 I(X ≥ 1)$. (Note that $I(X ≥ 1)$ equals 1 for $X ≥ 1$ and 0 otherwise.) We fit the linear regression model\n",
    "$$Y= β_0 + β_1b_1(X) + β_2b_2(X) + ϵ,$$\n",
    "and obtain coefficient estimates $\\hat{β}_0 = 1, \\hat{β}_1 = 1, \\hat{β}_2 =−2$. Sketch the estimated curve between $X=−2 $ and $X = 2$. Note the intercepts, slopes, and other relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we fit a curve with basis functions $b_1(X) = I(0 ≤ X ≤ 2)−(X− 1)I(1 ≤ X ≤ 2), b2(X) = (X− 3)I(3 ≤ X ≤ 4) + I(4 < X ≤ 5).$\n",
    "We fit the linear regression model\n",
    "$$Y= β_0 + β_1b_1(X) + β_2b_2(X) + ϵ,$$\n",
    "and obtain coefficient estimates $\\hat{β}_0 = 1,\\hat{β}_1 = 1, \\hat{β}_2 = 3.$ Sketch the estimated curve between $X=−2$ and $X = 6$. Note the intercepts, slopes, and other relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two curves, $\\hat{g}_1$ and $\\hat{g}_2$ defined by\n",
    "$$ \\hat{g}_1 = \\text{ arg min}_g \\large(\\sum_{i=1}^n(y_i-g(x_i))^2 + \\lambda \\int[g^{(3)}(x)]^2 dx\\large),$$\n",
    "$$ \\hat{g}_2 = \\text{ arg min}_g \\large(\\sum_{i=1}^n(y_i-g(x_i))^2 + \\lambda \\int[g^{(4)}(x)]^2 dx\\large),$$\n",
    "where $g^{(m)}$ represents the $m$ th derivative of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $λ → ∞$, will $\\hat{g}_1$ or $\\hat{g}_2$ have the smaller training RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $λ → ∞$, will $\\hat{g}_1$ or $\\hat{g}_2$ have the smaller test RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $λ =0$, will $\\hat{g}_1$ or $\\hat{g}_2$ have the smaller training and test RSS?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
